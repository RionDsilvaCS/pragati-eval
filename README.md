# **Pragati Eval: A Free LLM Evaluation Platform for Developers**

## **Introduction**

Pragati Eval is a free and open-source platform designed to help developers, especially those new to AI, easily evaluate the performance of different Large Language Models (LLMs) for their specific use cases. Inspired by Maxim AI, Pragati Eval offers a user-friendly interface for streamlined LLM experimentation.

## **Features**

### **Currently Available:**

* **Single Prompt Evaluation:**
    * Test a single prompt against multiple LLMs (e.g., GPT-3, Bard, Llama 2)
    * Analyze LLM responses for effective tool usage (data stores, extensions, functions)
* **User-Friendly Interface:** Intuitive and easy-to-use for developers of all levels.

### **Coming Soon:**

* **Prompt Comparison:**
    * Evaluate a single model's performance across multiple prompts.
    * Compare the performance of multiple models across multiple prompts.
* **Evaluation Datasets:** Test LLM performance on predefined datasets for specific tasks.
* **Prompt Chains:** Evaluate the effectiveness of complex prompt chains.
* **Workflow Evaluation:** Assess the performance of entire LLM-powered workflows.

## **Goals**

Pragati Eval aims to empower developers, both AI and non-AI engineers, to:

* **Select the best LLM:** Easily identify the most suitable LLM for their specific needs and use cases.
* **Improve LLM performance:** Gain insights into how to optimize prompts and leverage tools effectively.
* **Accelerate development:** Streamline the LLM evaluation process and reduce development time.
